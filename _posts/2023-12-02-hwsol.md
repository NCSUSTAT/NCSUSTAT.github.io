---
title: "Project Report: Independent Study of Automation and Robotics using UR Robot (ISE-837)"
author: "Yongseok Jeon, Kyoung Min Kim\n\nEdward P. Fitts Department of Industrial and Systems Engineering\n\nSupervisor: Dr. Yuanshin Lee, Dr. Ola Harryson, Dr. Jingyan Dong, Dr. Rohan Shirwaiker"
output:
  html_document:
    toc: true
    toc_depth: 2
    
header-includes:
- \usepackage{kotex,amssymb,amsmath,amsfonts, verbatim}


---
# Explain
This sentence uses `$` delimiters to show math inline:  $\sqrt{3x-1}+(1+x)^2$


Let $M \subset X$ be a  closed subspace in an inner product space s.t. $M  \neq M^{\perp \perp}$.\\
(Note that this would not be possible in a Hilbert space!!!) \\
Let $x \in 
M^{\perp \perp} - M$. Show that there is no best approximation to $x$ in M.\\
(proof) Fix $x \in M^{\perp \perp} - M$.
Suppose, to the contrary, that there exists best approximation to $x$ in M, say $y_0 \in M$. That is, $\exists y_0 \in M$ s.t. $\inf_{y \in M}||x-y||=||x-y_0||$. We can prove that $x-y_0 \in M^{\bot}$ using the proof in Lemma 3.3-2. From the fact that $x \in M^{\bot \bot}$,\begin{equation} <x-y_o,x>=0 \end{equation}
From the fact that $y_0 \in M$, \begin{equation} <x-y_0,y_0>=0 \end{equation}
From(1),(2), it follows that 
\begin{equation} ||x-y_o||^2=<x-y_0,x-y_0>=0 \Longleftrightarrow x=y_0 \end{equation}
, which contradicts to the fact that $y_0 \in M$, $x \notin M$.(Note that $x$ is non-zero vector).

Show that $\{\sin nx\}_{n \geq 1}$ and $\{\cos nx\}_{n \geq 1}$ are orthogonal in $L^2[-\pi,\pi]$. Build two orthonormal sequences in $L^2[-\pi,\pi]$.\\
(Answer)Let $L^2[-\pi,\pi]$ be an inner product space of all real-valued functions with inner product defined by $<x,y>=\int_{-\pi}^{\pi}x(t)y(t)dt$. Then we can build two orthonormal sequences in $L^2[-\pi,\pi]$.\\1)$\{  \frac{1}{\sqrt{2\pi}},\frac{sin(nx)}{\sqrt{\pi}} \}_{1}^{\infty}$\\
1)-1(orthogonality):$<1,sin(nx)>=\int_{-\pi}^{\pi}sin(nt)dt=[-\frac{con(nx)}{n}]_{-\pi}^{\pi}=0$\\1)-2:$||\frac{1}{\sqrt{2\pi}}||^2=<\frac{1}{\sqrt{2\pi}},\frac{1}{\sqrt{2\pi}}>=\int_{-\pi}^{\pi}\frac{1}{2\pi}dt=1$,$||\frac{sin^2(nx)}{\pi}||^2=<             \frac{sin(nx)}{\sqrt{\pi}},\frac{sin(nx)}{\sqrt{\pi}}>=\int_{-\pi}^{\pi}\frac{sin^2(nt)}{\pi}dt=\int_{-\pi}^{\pi}\frac{1-cos(2nx)}{2\pi}dx=[\frac{0.5}{\pi}(x-\frac{sin(2nx)}{2n})]_{-\pi}^{\pi}=1$


2)$\{  \frac{1}{\sqrt{2\pi}},\frac{cos(nx)}{\sqrt{\pi}} \}_{1}^{\infty}$\\
2)-1(orthogonality):$<1,cos(nx)>=\int_{-\pi}^{\pi}cos(nt)dt=[\frac{sin(nx)}{n}]_{-\pi}^{\pi}=0$\\2)-2:$||\frac{1}{\sqrt{2\pi}}||^2=<\frac{1}{\sqrt{2\pi}},\frac{1}{\sqrt{2\pi}}>=\int_{-\pi}^{\pi}\frac{1}{2\pi}dt=1$,$||\frac{cos^2(nx)}{\pi}||^2=<             \frac{cos(nx)}{\sqrt{\pi}},\frac{cos(nx)}{\sqrt{\pi}}>=\int_{-\pi}^{\pi}\frac{cos^2(nt)}{\pi}dt=\int_{-\pi}^{\pi}\frac{1+cos(2nx)}{2\pi}dx=[\frac{0.5}{\pi}(x+\frac{sin(2nx)}{2n})]_{-\pi}^{\pi}=1$


Determine whether or not the following is true in a Hilbert space H: 
$$[x \perp y ]\ \  \text{iff}\ \  [\|x+y\|^2 = \|x\|^2 + \|y\|^2].$$
\\
$(\Rightarrow)$ True \\
$||x+y||^2=<x+y,x+y>=||x||^2+<x,y>+<y,x>+||y||^2=||x||^2+||y||^2$. The last equality follows from the assumption of $x \bot y$. 
\\
$(\Leftarrow)$ False \\
For $\mathbb{C}^2$, which is a complex inner product space, let $y:=ix$. Then, it follows that \begin{equation} ||y||=||ix||=||x|| \end{equation}
Note that \begin{equation} ||x+y||^2=||x+ix||^2=|1+i|^2||x||^2=2||x||^2\end{equation}
From(4),(5), it follows that\begin{equation} ||x+y||^2=||x||^2+||y||^2\end{equation}
However,\begin{equation} <x,y>=<x,ix>=\overline{i}<x,x>=-i<x,x>=-i||x||^2\neq0\end{equation}



Let $X$ be a n.s., $Y$ a subspace in $X$, and $x \in X$. Show that the set $M$ of best approximations to $x$ out of $Y$ is convex, i.e., $\al x + (1-\al)y \in  M$, for all $x,y \in M$ and $\al \in [0,1]$.\\
(Answer)Let X be a normed space with norm $||.||$. Fix $x \in X$, and let $\delta=\inf_{y \in Y}||x-y||$. Let $M$(Best approximation to x out of Y):=$\{z \in Y| \delta=||x-z||\}$. Then, 
$\forall k,z \in M$, and $\alpha \in [0,1]$,\begin{equation} ||x-(\alpha k+(1-\alpha)z)||=||\alpha(x-k)+(1-\alpha)(x-z)||\leq |\alpha|||x-k||+|1-\alpha|||x-z||=\alpha \delta+(1-\alpha)\delta=\delta \end{equation}
Since Y is  a subspace in X, $\alpha k+(1-\alpha)z \in M$ and, thus, \begin{equation} \delta \leq ||x-(\alpha k+(1-\alpha)z)|| \end{equation} 
From (8),(9), it follows that \begin{equation}\delta =||x-(\alpha k+(1-\alpha)z)||
\end{equation}, that is, $\alpha k+(1-\alpha)z \in M$, $\forall \alpha \in [0,1]$ and $\forall k,z \in M$.


Show that a Hilbert space $H$ is strictly convex, i.e. for any $x,y \in H$ with $\|x \| = \|y\| = 1$, we have $\|x+y\| < 2$.\\
(Answer)
Let $v,w \in H, \forall \alpha \in (0,1)$.Then, 
    $\alpha||v||^2+(1-\alpha)||w||^2-||\alpha v+(1-\alpha)w||^2
   = \alpha||v||^2+(1-\alpha)||w||^2-(\alpha^2||v||^2+(1-\alpha)^2||w||^2+2Re<\alpha v,(1-\alpha)w>)=(\alpha-\alpha^2)||v||^2+(1-\alpha)(1-(1-\alpha))||w||^2-2Re<\alpha v,(1-\alpha)w>)=\alpha(1-\alpha)||v||^2+(1-\alpha)(\alpha)||w||^2-2Re\ \alpha(1-\alpha)<v,w>)=\alpha(1-\alpha)[||v||^2+||w||^2-2 Re<v,w>]=\alpha(1-\alpha)||v-w||^2>0$, if $v \neq w$. That is,  $\alpha||v||^2+(1-\alpha)||w||^2 >||\alpha v+(1-\alpha)w||^2$, $\forall \alpha \in (0,1)$,$v,w \in H$. For $\alpha=\frac{1}{2}$, it follows that  $\frac{1}{2}||v||^2+\frac{1}{2}||w||^2 >||\frac{1}{2} v+\frac{1}{2}w||^2 \Longleftrightarrow 2>||v+w||$, $v,w \in H$.



Find the best approximation to $\sin(x)$ in $L^2[0,1]$ by a polynomial of degree $\leq 3$. 
(You can use software to help with the calculations if necessary; or you can leave coefficients as inner products when applicable). \\(Answer)
Let $L^2[0,1]$ be an inner product space with inner product defined by $<x,y>=\int_{0}^{1}x(t)y(t)dt, \forall x,y \in L^2[0,1]$.Consider the set $A:=\{x_0(t),x_1(t),x_2(t),x_3(t)\}$, where $x_i(t)=t^i,\forall i \in \{0,1,2,3\}$. Then, the set A is linearly independent, and we can construct an orthonormal set from A by the Gram-Schmidt process.\\ 1)constructing an orthonormal set $\{e_1,e_2,e_3,e_4\}$\\
1)-1:\\
$e_1=\frac{1}{||1||}=1$, then $||e_1||^2=<1,1>=\int_{0}^{1}dt=1$\\
1)-2:\\
$e_2=\frac{x_2-<x_2,e_1>e_1}{||x_2-<x_2,e_1>e_1||}=\frac{t-\frac{1}{2}}{\sqrt{\frac{1}{12}}}=2\sqrt{3}(t-\frac{1}{2})$ from the fact that $<x_2,e_1>=\int_{0}^{1}tdt=\frac{1}{2}$, $x_2-<x_2,e_1>e_1=t-\frac{1}{2}$, and $||x_2-<x_2,e_1>e_1 ||^2=<x_2-<x_2,e_1>e_1,x_2-<x_2,e_1>e_1>=<t-\frac{1}{2},t-\frac{1}{2}>=\frac{1}{12}$.
1)-3:\\
$e_3=\frac{x_3-<x_3,e_1>e_1-<x_3,e_2>e_2}{||x_3-<x_3,e_1>e_1-<x_3,e_2>e_2||}=6\sqrt{5}(t^2-t+\frac{1}{6})$ from the fact that $<x_3,e_1>=\int_{0}^{1}t^2dt=\frac{1}{3}$,$<x_3,e_2>=\int_{0}^{1}t^2 2\sqrt{3}(t-\frac{1}{2}) dt=\frac{\sqrt{3}}{6}$, $x_3-<x_3,e_1>e_1-<x_3,e_2>e_2 =t^2-t+\frac{1}{6} $, and $||x_3-<x_3,e_1>e_1-<x_3,e_2>e_2||^2=<x_3-<x_3,e_1>e_1-<x_3,e_2>e_2,x_3-<x_3,e_1>e_1-<x_3,e_2>e_2>=<t^2-t+\frac{1}{6},t^2-t+\frac{1}{6}>=\frac{1}{180}$.\\
1)-4:\\
$e_4=\frac{x_4-<x_4,e_1>e_1-<x_4,e_2>e_2-<x_4,e_3>e_3}{||x_4-<x_4,e_1>e_1-<x_4,e_2>e_2-<x_4,e_3>e_3||}=\frac{(t^3+\frac{3}{2}t^2-\frac{12}{5}t+\frac{9}{20})}{0.2244}$ from the fact that $<x_4,e_1>=\int_{0}^{1}t^3dt=\frac{1}{4}$,$<x_4,e_2>=\int_{0}^{1} 2\sqrt{3}t^3(t-\frac{1}{2}) dt=\frac{6\sqrt{3}}{40},<x_4,e_3>=\int_{0}^{1}6\sqrt{5}t^3(t^2-t+\frac{1}{6}) dt=-\frac{\sqrt{5}}{20} $, $x_4-<x_4,e_1>e_1-<x_4,e_2>e_2-<x_4,e_3>e_3 =t^3+\frac{3}{2}t^2-\frac{12}{5}t+\frac{9}{20} $, and $||x_4-<x_4,e_1>e_1-<x_4,e_2>e_2-<x_4,e_3>e_3||^2=<x_4-<x_3,e_1>e_1-<x_4,e_2>e_2-<x_4,e_3>e_3,x_4-<x_4,e_1>e_1-<x_4,e_2>e_2-<x_4,e_3>e_3=<t^3+\frac{3}{2}t^2-\frac{12}{5}t+\frac{9}{20},t^3+\frac{3}{2}t^2-\frac{12}{5}t+\frac{9}{20}>=0.05036$.\\
2)Best Approximation to sin(x)\\
By problem 6 in 3.4(p159), best approximation to sin(x) is $<sin(x),e_1>e_1+<sin(x),e_2>e_2+<sin(x),e_3>e_3+<sin(x),e_4>e_4$, where $\{e_1,e_2,e_3,e_4\}=\{1,2\sqrt{3}(t-\frac{1}{2}),6\sqrt{5}(t^2-t+\frac{1}{6}),\frac{(t^3+\frac{3}{2}t^2-\frac{12}{5}t+\frac{9}{20})}{0.2244} \},\ <sin(x),e_1>=\int_{0}^{1}sin(x)dx, \ \ <sin(x),e_2>=\int_{0}^{1}sin(x)2\sqrt{3}(x-\frac{1}{2})dx,\ \ <sin(x),e_3>=\int_{0}^{1}sin(x)6\sqrt{5}(x^2-x+\frac{1}{6})dx$,\ \ and $<sin(x),e_4>=\int_{0}^{1}sin(x)\frac{(x^3+\frac{3}{2}x^2-\frac{12}{5}x+\frac{9}{20})}{0.2244}dx$.


Let $H$ be a H.s. and  $M \subset H$. Prove that
 $$M \ \ \text{is total iff} \ \ [x \perp M \Rightarrow \ x = 0] $$\\
 $(\Rightarrow)$ $M$ is total in $H$ implies that the span of $M$ is dense in $H$. By Lemma 3.3-7, it follows that $M^{\bot}=\{0\}$. That is, if $x \bot M$, it follows that $x=0$.\\
 $(\Leftarrow)$ From the assumption, it follows that $M^{\bot}=\{0\}$. Then, by Lemma 3.3-7, we can conclude that $M$ is total.

 \item p. 159/7  \\ 
 From Holder inequality (14p(10)), we get \begin{equation} \sum_{k=1}^{\infty}|<x,e_k><y,e_k>|\leq (\sum_{k=1}^{\infty}|<x,e_k>|^2)^{\frac{1}{2}}(\sum_{j=1}^{\infty}|<y,e_j>|^2)^{\frac{1}{2}}
\end{equation}
From Thm3.4-6(Bessel Inequality), it follows that \begin{equation} (\sum_{k=1}^{\infty}|<x,e_k>|^2)^{\frac{1}{2}}(\sum_{j=1}^{\infty}|<y,e_j>|^2)^{\frac{1}{2}}
\leq ||x||||y|| \end{equation}
Combining (11),(12), we get  \begin{equation*} \sum_{k=1}^{\infty}|<x,e_k><y,e_k>|\leq (\sum_{k=1}^{\infty}|<x,e_k>|^2)^{\frac{1}{2}}(\sum_{j=1}^{\infty}|<y,e_j>|^2)^{\frac{1}{2}}
\leq ||x||||y|| \end{equation*}
 
p. 159/10  \\ 
 Consider the set $A:=\{x_1(t),x_2(t),x_3(t)\}$, where $x_1(t)=t^2, x_2(t)=t, x_3(t)=1$. Then, the set A is linearly independent, and we can construct an orthonormal set from A by the Gram-Schmidt process.\\ $\bullet$ constructing an orthonormal set $\{e_1,e_2,e_3\}$\\
9)-1:\\
$e_1=\frac{x_1}{||x_1||}=\frac{x_1}{\sqrt{\frac{2}{5}}}=\frac{\sqrt{5}}{\sqrt{2}}t^2$, from the fact that $||x_1||^2=<x_1,x_1>=\int_{-1}^{1}x_1(t)dt=\int_{-1}^{1} t^4dt=\frac{2}{5}$\\
9)-2:\\
$e_2=\frac{x_2-<x_2,e_1>e_1}{||x_2-<x_2,e_1>e_1||}=\frac{x_2}{||x_2||}=\frac{\sqrt{3}t}{
\sqrt{2}}$ from the fact that $<x_2,e_1>=\int_{-1}^{1}(\frac{\sqrt{5}}{\sqrt{2}}t^3)dt=0$, $x_2-<x_2,e_1>e_1=t$, and $||x_2-<x_2,e_1>e_1 ||^2=<x_2-<x_2,e_1>e_1,x_2-<x_2,e_1>e_1>=<t,t>=\frac{2}{3}$.\\
9)-3:\\
$e_3=\frac{x_3-<x_3,e_1>e_1-<x_3,e_2>e_2}{||x_3-<x_3,e_1>e_1-<x_3,e_2>e_2||}=\frac{3}{2\sqrt{2}}(1-\frac{5t^2}{3})$ from the fact that $<x_3,e_1>=\int_{-1}^{1}\frac{\sqrt{5}t^2}{\sqrt{2}}dt=\frac{\sqrt{10}}{3}$,$<x_3,e_2>=\int_{-1}^{1} \frac{\sqrt{3}t}{\sqrt{2}} dt=0$, $x_3-<x_3,e_1>e_1-<x_3,e_2>e_2 =1-\frac{5t^2}{3} $, and $||x_3-<x_3,e_1>e_1-<x_3,e_2>e_2||^2=<x_3-<x_3,e_1>e_1-<x_3,e_2>e_2,x_3-<x_3,e_1>e_1-<x_3,e_2>e_2>=<1-\frac{5t^2}{3},1-\frac{5t^2}{3}>=\frac{8}{9}$.\\
That is, $\{\frac{\sqrt{5}}{\sqrt{2}}t^2,\frac{\sqrt{3}t}{
\sqrt{2}},\frac{3}{2\sqrt{2}}(1-\frac{5t^2}{3})\}$ is an orthonormal set obtained from A.
 
 
 
 
 
 
 
 
 p.175 / 9 \\ 
 $<v,x>=<w,x>, \forall x \in M \Longleftrightarrow$ $<v-w,x>=0, \forall x \in M$. Since M is a total set in inner product space X, it follows that $v-w=0 \Longleftrightarrow v=w$.

 p.175 / 10 \\
(Given Assumption):For some $v,w \in H$, $<v,x>=<w,x>,\forall x \in M$ implies $v=w$.\\
$\Longleftrightarrow$ For some $y \in H$, $<y,x>=0,\forall x \in M$ implies $y=0$.\\
$\Longleftrightarrow$ $M$ is total in $H$, by Thm3.6-2 (b).

Prove that every v.s. $X$ has a Hamel basis.\\
 (proof)Let $M$ be the set of all linearly independent subset of $X$.Since $X \neq 0$, it follows that $M \neq 0$.Note that set inclusion defines a partial ordering on $M$, and that every chain $C \subset M$ has an upper bound, which is union of all subsets of $X$.By Zorn's Lemma, $M$ has a maximal element, say B.\\(claim) B is Hamel basis for $X$, that is, $span B=Y$ and $Y=X$.\\(proof of claim) Suppose, to the contrary, that $Y \subsetneq X$. Then, $\exists z \in X$ s.t. $z \notin Y$. It follows that $B \cup \{z\}$ is the linearly independent set containing $B$ as a proper subset, contrary to the maximality of $B$.
 
 Prove Hahn-Banach Th (Real Version). Use the ideas discussed in class.\\
 (Hahn-Banach Thm) Let $X$ be a real vector space and $p$ a sublinear functional on $X$. Furthermore, let $f$ be a linear functional which is defined on a subspace $Z$ of $X$ and 
 satisfies \begin{equation} f(x) \leq p(x), \forall x \in Z \end{equation}.
 Then, $f$ has a linear extension $\Tilde{f}$ from  $Z$ to $X$ satisfying \begin{equation}
\Tilde{f}(x) \leq p(x), \forall x \in X \end{equation}\\
That is, $\Tilde{f}$ is a linear functional on $X$, satisfies (14) on $X$ and $\Tilde{f}(x)=f(x)$, for every $x \in Z$. \\
 (proof)Proceeding stepwise, we shall prove:\\
 (a) The set $E$ of all linear extensions $g$ of $f$ satisfying $g(x) \leq p(x)$ on their domain $D(g)$ can be partially ordered and Zorn's lemma yields a maximal element $\Tilde{f}$ of $E$.\\
 (b) $\Tilde{f}$ is defined on the entire space $X$.\\
 (c) An auxiliary relation which was used in (b).\\ \\
 We start with part \\
 (a)Let $E$ be the set of all linear extensions $g$ of $f$ which satisfies the condition \begin{equation} g(x) \leq p(x), \forall x \in D(g) \end{equation}
 Clearly,$E \neq \varnothing$, since $f \in E$. On $E$, we can define a partial ordering by 
 $g \leq h$ implies $h$ is an extension of $g$. Then, for any chain $C \subset E$, we can set an upper bound of $C$, by $\hat{g}(x)=g(x)$, if $x \in D(g)$ for $g \in C$, and $D(\hat{g})=\cup_{g \in C}D(g)$. It follows that $g \leq \hat{g}, \forall g \in C$. By Zorn's Lemma, $E$ has a maximal element $\Tilde{f}$. By the definition of $E$, this is a linear extension of $f$ which satisfies $\Tilde{f}(x) \leq p(x), \forall x \in D(\Tilde{f}) $.\\
 (b)We now show that $D(\Tilde{f})$ is all of $X$. Suppose, to the contrary, that this is false. Then, we can choose a $y_1 \in X-D(\Tilde{f})$ and consider the subspace $Y_1$ of $X$ spanned by $D(\Tilde{f})$ and $y_1$. Any $x \in Y_1$ can be written $x=y+\alpha y_1,y\in D(\Tilde{f})$. we can prove uniqueness of this expression.\\
 Define functional $g_1$ on $Y_1$ as \begin{equation} g_1(y+\alpha y_1)=\Tilde{f}(y)+ \alpha c, c \in \mathbb{R}\end{equation} Then, $g_1$ is a proper extension of $\Tilde{f}$. We will prove in (c) that $g_1 \in E$ by showing that \begin{equation} g_1(x) \leq p(x), \forall x \in D(g_1)\end{equation} Hence, (17) will contradict the maximality of $\Tilde{f}$, so that $D(f)=X$.\\
 (c)Accordingly, we will show that $g_1$ with a suitable $c$ in (16) satisfies (17).\\
 For  any $y$ and $z$ in $D(\Tilde{f})$, we get $\Tilde{f}(y)=\Tilde{f}(z) \leq p(y+y_1)+p(-y_1-z)$ from (14) and the fact that $p$ is sub-linear on $X$. Equivalently, it follows that \begin{equation}-p(-y_1-z)-\Tilde{f}(z) \leq p(y+y_1)-\Tilde{f}(y)
 \end{equation} By taking supremum over $z \in D(\Tilde{f})$ on the right, and the infimum over $y \in D(\Tilde{f})$ on the right, it follows that \begin{equation} -p(-y_1-z)-\Tilde{f}(z) \leq c \end{equation} and 
 \begin{equation} c \leq  p(y+y_1)-\Tilde{f}(y) \end{equation}\\
 Consider the case when $\alpha>0$,$\alpha<0$, and $\alpha=0$. we can show that (17) holds.  
     

 
